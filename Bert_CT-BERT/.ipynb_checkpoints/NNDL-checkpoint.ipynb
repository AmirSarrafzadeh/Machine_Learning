{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65505633-6b77-41d3-afee-727a3a794420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as functional\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import gc\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a9583b-c3f8-4488-838e-924f36bc717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'Q1\\Data'\n",
    "train_data_path = os.path.join(data_path, 'Train.csv')\n",
    "val_data_path = os.path.join(data_path, 'Val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c453ddb1-67ec-428c-bfa9-e88b065afd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_data_path)\n",
    "val = pd.read_csv(val_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2d7e1e-3aa6-4085-be7c-9d0fd97db0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"label\"] = train[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
    "val[\"label\"] = val[\"label\"].map({\"real\": 1, \"fake\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40e8af0-1077-45ad-a364-66d03d6a551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8555</th>\n",
       "      <td>Donald Trump wrongly claimed that New Zealand ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8556</th>\n",
       "      <td>Current understanding is #COVID19 spreads most...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8557</th>\n",
       "      <td>Nothing screams “I am sat around doing fuck al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8558</th>\n",
       "      <td>Birx says COVID-19 outbreak not under control ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8559</th>\n",
       "      <td>Another 4422 new coronavirus cases have been c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  label\n",
       "0     The CDC currently reports 99031 deaths. In gen...      1\n",
       "1     States reported 1121 deaths a small rise from ...      1\n",
       "2     Politically Correct Woman (Almost) Uses Pandem...      0\n",
       "3     #IndiaFightsCorona: We have 1524 #COVID testin...      1\n",
       "4     Populous states can generate large case counts...      1\n",
       "...                                                 ...    ...\n",
       "8555  Donald Trump wrongly claimed that New Zealand ...      0\n",
       "8556  Current understanding is #COVID19 spreads most...      1\n",
       "8557  Nothing screams “I am sat around doing fuck al...      0\n",
       "8558  Birx says COVID-19 outbreak not under control ...      0\n",
       "8559  Another 4422 new coronavirus cases have been c...      1\n",
       "\n",
       "[8560 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([train, val], axis=0, ignore_index=True).drop([\"id\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73766922-e8c8-4a48-8100-69443687a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a4eec2-d12c-4546-98c9-e8b98db829bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.tweet.values\n",
    "labels = data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48e97b6-ebc6-40b0-aee0-c3b8e2ed570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the cdc currently reports 99031 deaths in general the discrepancies in death counts between different sources are small and explicable the death toll stands at roughly 100000 people today '\n",
      " 'states reported 1121 deaths a small rise from last tuesday southern states reported 640 of those deaths '\n",
      " 'politically correct woman almost uses pandemic as excuse not to reuse plastic bag '\n",
      " ...\n",
      " 'nothing screams “i am sat around doing fuck all during lockdown” quite like confident assumption that other people are sat around doing fuck all during lockdown '\n",
      " 'birx says covid19 outbreak not under control because ‘people are on the move’ '\n",
      " 'another 4422 new coronavirus cases have been confirmed in the uk the highest daily number since 8 may its up from 4322 new cases reported on friday and the overall total nationwide now stands at 385936 read the latest here ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "def preprocess(data):\n",
    "    #remove url and hashtag\n",
    "    for i in range(data.shape[0]):\n",
    "        text=data[i].lower()\n",
    "        text1=''.join([word+\" \" for word in text.split()])\n",
    "        data[i]=text1\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    space_pattern = '\\s+'\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        text_string = data[i]\n",
    "        parsed_text = re.sub(hashtag_regex, '', text_string)\n",
    "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "        parsed_text = re.sub(mention_regex, '', parsed_text) \n",
    "        #remove punctuation\n",
    "        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text) \n",
    "        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
    "        data[i] = parsed_text\n",
    "    return data\n",
    "tweets = preprocess(tweets)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372533a6-e693-41ed-b942-c3844fa93480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  the cdc currently reports 99031 deaths in general the discrepancies in death counts between different sources are small and explicable the death toll stands at roughly 100000 people today \n",
      "Tokenized:  ['the', 'cdc', 'currently', 'reports', '99', '##0', '##31', 'deaths', 'in', 'general', 'the', 'disc', '##re', '##pan', '##cies', 'in', 'death', 'counts', 'between', 'different', 'sources', 'are', 'small', 'and', 'ex', '##pl', '##ica', '##ble', 'the', 'death', 'toll', 'stands', 'at', 'roughly', '1000', '##00', 'people', 'today']\n",
      "Token IDs:  [1996, 26629, 2747, 4311, 5585, 2692, 21486, 6677, 1999, 2236, 1996, 5860, 2890, 9739, 9243, 1999, 2331, 9294, 2090, 2367, 4216, 2024, 2235, 1998, 4654, 24759, 5555, 3468, 1996, 2331, 9565, 4832, 2012, 5560, 6694, 8889, 2111, 2651]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', tweets[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08a966e-cb3d-42a8-acac-54d4f233baab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of sentence length over 100 is:  5667\n",
      "Count of sentence length over 200 is:  2327\n",
      "Count of sentence length over 300 is:  59\n",
      "Count of sentence length over 400 is:  20\n",
      "Count of sentence length over 500 is:  13\n",
      "Count of sentence length over 512 is:  10\n",
      "Max sentence length:  8672\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "ind = [100,200,300,400,500,512]\n",
    "for i in ind:\n",
    "  count = 0\n",
    "  for tweet in tweets:\n",
    "      max_len = max(max_len, len(tweet))\n",
    "      if len(tweet)>i:\n",
    "        count+=1\n",
    "  print(\"Count of sentence length over {} is: \".format(i), count)\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0552e9-8a81-4b18-b30f-d91298af4453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  the cdc currently reports 99031 deaths in general the discrepancies in death counts between different sources are small and explicable the death toll stands at roughly 100000 people today \n",
      "Token IDs: tensor([  101,  1996, 26629,  2747,  4311,  5585,  2692, 21486,  6677,  1999,\n",
      "         2236,  1996,  5860,  2890,  9739,  9243,  1999,  2331,  9294,  2090,\n",
      "         2367,  4216,  2024,  2235,  1998,  4654, 24759,  5555,  3468,  1996,\n",
      "         2331,  9565,  4832,  2012,  5560,  6694,  8889,  2111,  2651,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "for tweet in tweets:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 512,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', tweets[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bfbeaca-a9dc-4e2a-a9cd-5a690c94425a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,704 training samples\n",
      "  856 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a449f0-edd0-44b7-8af3-98df6d24272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            shuffle = True,\n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            shuffle = False,\n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854b71ef-17ce-435e-bcc1-efc55f567f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b95163-a10a-4e52-8b81-e29cc51f7a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2, \n",
    "    output_attentions = False, \n",
    "    output_hidden_states = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb79a4af-2c8c-4e46-9f00-ea07b3726c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AMIR\\Python\\amir\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "epochs = 4\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7d30f-6c35-4ee0-85c6-e67a86a5886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 4\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "best_accuracy = 0\n",
    "for epoch_i in range(0, epochs):\n",
    "    #Training\n",
    "    print(\"\")\n",
    "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        input_ids = batch[0]\n",
    "        input_mask = batch[1]\n",
    "        labels = batch[2]\n",
    "\n",
    "        model.zero_grad()        \n",
    "        out = model(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n",
    "        loss = out[0]\n",
    "        logits = out[1]\n",
    "  \n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(logits, dim = 1)\n",
    "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
    "        \n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)            \n",
    "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
    "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
    "        \n",
    "\n",
    "    # Validation\n",
    "    print(\"\")\n",
    "    print(\"Validation...\")\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            out = model(input_ids, token_type_ids=None, attention_mask=input_mask,labels=labels)\n",
    "            loss = out[0]\n",
    "            logits = out[1]\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "        pred = torch.argmax(logits, dim = 1)\n",
    "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
    "        y_true.append(labels.flatten())\n",
    "        y_pred.append(pred.flatten())\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
    "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print()\n",
    "    \n",
    "    y_true = torch.cat(y_true).tolist()\n",
    "    y_pred = torch.cat(y_pred).tolist()\n",
    "    print(\"This epoch took: {:}\".format(training_time))\n",
    "    print('roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
    "    print('F1 score:',f1_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Train Accur.': avg_train_accuracy,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    if avg_val_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_val_accuracy\n",
    "        best_model = model\n",
    "\n",
    "print()\n",
    "print(\"=\"*10)\n",
    "print(\"Summary\")\n",
    "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bd24d-cefb-43fa-b6eb-9e5016bac05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = os.getcwd()\n",
    "torch.save(model, PATH1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2addb39-fcb3-4dc1-a9a5-fe13e71edbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLstmClassifier(nn.Module):\n",
    "    def __init__(self, model_tune):\n",
    "        super().__init__()\n",
    "        self.bert = model_tune.bert\n",
    "        self.lstm = nn.LSTM(input_size = 768, \n",
    "                            hidden_size = 768, \n",
    "                            num_layers = 1, \n",
    "                            batch_first = True, \n",
    "                            bidirectional = True)\n",
    "        self.classifier = nn.Linear(768 * 2, 2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        out, _ = self.lstm(bert_output[0])\n",
    "        logits = self.classifier(out[:, 1, :])\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a311f4-8718-4826-8ac0-2939b28f7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "model4 = BertLstmClassifier(the_best_model).cuda()\n",
    "# set parameters\n",
    "epochs = 6\n",
    "learning_rate = 5e-5\n",
    "optimizer = AdamW(model4.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc21c51-ef73-420e-aa37-e84f7131fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "best_accuracy = 0\n",
    "for epoch_i in range(0, epochs):\n",
    "    #Training\n",
    "    print(\"\")\n",
    "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    model4.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        model4.zero_grad()        \n",
    "        out = model4(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
    "        loss = criterion(out, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model4.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(out, dim = 1)\n",
    "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
    "        \n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)            \n",
    "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
    "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    print(\"\")\n",
    "    print(\"Validation...\")\n",
    "    model4.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            out = model4(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
    "        loss = criterion(out, labels)\n",
    "        total_eval_loss += loss.item()\n",
    "        pred = torch.argmax(out, dim = 1)\n",
    "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
    "        y_true.append(labels.flatten())\n",
    "        y_pred.append(pred.flatten())\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
    "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"  This epoch took: {:}\".format(training_time))\n",
    "    print()\n",
    "    y_true = torch.cat(y_true).tolist()\n",
    "    y_pred = torch.cat(y_pred).tolist()\n",
    "    print('  roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
    "    print('  F1 score:',f1_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Train Accur.': avg_train_accuracy,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if avg_val_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_val_accuracy\n",
    "        best_model = model4\n",
    "\n",
    "print(\"===\")\n",
    "print(\"Summary\")\n",
    "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "print('best acc:',best_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce392838-2ddc-48e9-b65e-87b34458de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH4 = os.path.join(os.getcwd(), \"lstm.pt\")\n",
    "torch.save(model4, PATH4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2ea09-1cb4-49c2-8bfc-e3713580d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLstmClassifier(nn.Module):\n",
    "    def __init__(self, model_tune):\n",
    "        super().__init__()\n",
    "        self.bert = model_tune.bert\n",
    "        self.lstm = nn.LSTM(input_size = 768, \n",
    "                            hidden_size = 768, \n",
    "                            num_layers = 2, \n",
    "                            batch_first = True, \n",
    "                            bidirectional = True)\n",
    "        self.classifier = nn.Linear(768 * 2, 2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        out, _ = self.lstm(bert_output[0])\n",
    "        logits = self.classifier(out[:, 1, :])\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62375829-0da9-4b26-a956-1d499e80b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = BertLstmClassifier(the_best_model).cuda()\n",
    "for param in model5.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "# set parameters\n",
    "epochs = 10\n",
    "learning_rate = 5e-5\n",
    "optimizer = AdamW(model5.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3acfb-ebd7-4b73-9ec2-b27219586a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "best_accuracy = 0\n",
    "for epoch_i in range(0, epochs):\n",
    "    #Training\n",
    "    print(\"\")\n",
    "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    model5.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        model5.zero_grad()        \n",
    "        out = model5(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
    "        loss = criterion(out, labels)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model5.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(out, dim = 1)\n",
    "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
    "        \n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)            \n",
    "    print(\"  Accuracy: {}\".format(avg_train_accuracy))\n",
    "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
    "        \n",
    "    # Validation\n",
    "    print(\"\")\n",
    "    print(\"Validation...\")\n",
    "    model5.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        input_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            out = model5(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
    "        loss = criterion(out, labels)\n",
    "        total_eval_loss += loss.item()\n",
    "        pred = torch.argmax(out, dim = 1)\n",
    "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
    "        y_true.append(labels.flatten())\n",
    "        y_pred.append(pred.flatten())\n",
    "        \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
    "    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
    "    print(\"  Validation loss: {}\".format(avg_val_loss))\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"  This epoch took: {:}\".format(training_time))\n",
    "    print()\n",
    "    y_true = torch.cat(y_true).tolist()\n",
    "    y_pred = torch.cat(y_pred).tolist()\n",
    "    print('  roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
    "    print('  F1 score:',f1_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Train Accur.': avg_train_accuracy,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if avg_val_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_val_accuracy\n",
    "        best_model = model5\n",
    "\n",
    "print(\"===\")\n",
    "print(\"Summary\")\n",
    "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "print('best acc:',best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1558ab-4dfd-443b-938e-896ec8236fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH5 = os.path.join(os.getcwd(), \"lstm2.pt\")\n",
    "torch.save(model5, PATH5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26cc7d-db8d-47d3-b8d5-c44a4e07ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = \"/content/drive/Shareddrives/CS682 Project/'lstm2.pt'\"\n",
    "best_model = torch.load(PATH1,map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee44c0e-e771-4288-a481-fa0396b93b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model\n",
    "#model.to(device)\n",
    "model.eval()\n",
    "word_dict = {}\n",
    "for batch_index, batch in enumerate(val_dataloader):\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        token_type_ids = batch[2]\n",
    "        labels = batch[3]\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            res = model(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        # prediction\n",
    "        output = res[0]\n",
    "        pred = torch.argmax(output, dim = 1)\n",
    "        for i in pred:\n",
    "          if i ==1:\n",
    "            for j in range(input_ids.size(0)):\n",
    "              for idx in range(512):\n",
    "                if input_ids[j,idx].item()!=0:\n",
    "                  if input_ids[j,idx].item() not in word_dict:\n",
    "                      word_dict[input_ids[j,idx].item()] = 1\n",
    "                  else:\n",
    "                      word_dict[input_ids[j,idx].item()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393eba6-458b-46a1-a442-2ea28aab9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2 = sorted(word_dict.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b7092-b365-4515-9395-865f1abf3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(100):\n",
    "  word_list.append(dict2[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03e7ea-7efe-48a5-bacb-6707167cd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(word_list)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028cf3f-40eb-4861-8539-95ce9bc5c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drow word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS) \n",
    "all_text = \"\"\n",
    "for i in range(14,len(dict2)):\n",
    "    for j in range(dict2[i][1]):\n",
    "      words = tokenizer.convert_ids_to_tokens(dict2[i][0])\n",
    "      all_text += \" \" + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc6406-aa95-41c6-bf2c-365bacf48f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(collocations=False, width = 800, height = 300, stopwords = stopwords,background_color ='white',  min_font_size = 10)\n",
    "wordcloud.generate(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c57b7-cdb4-4cf5-9149-6d08621ff8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (8, 3), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674aea49-3f7b-419c-8b95-bc274391bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.mean(the_final_model.bert.embeddings.word_embeddings.weight,axis = 1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4bd195-ae9e-4e94-ad5d-8d35ddab936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted, indices = torch.sort(weights,descending = True)\n",
    "target_list = indices[0:200].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7cc88-7594-4215-a745-f4268a475d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(target_list)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0865c2-a4d3-4369-8163-b96705fc9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = indices[0:200].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427f9a3-48d3-4baf-b258-0fa03d20743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "L = []\n",
    "stopwords = set(STOPWORDS) \n",
    "string = set(string.punctuation)\n",
    "u = stopwords.union(string)\n",
    "for i in tokens:\n",
    "  if i not in u:\n",
    "    L.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092afb34-4482-46d3-a901-5e7a32cf2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLstmClassifier(nn.Module):\n",
    "    def __init__(self, model_tune):\n",
    "        super().__init__()\n",
    "        self.bert = model_tune.bert\n",
    "        self.lstm = nn.LSTM(input_size = 768, \n",
    "                            hidden_size = 768, \n",
    "                            num_layers = 2, \n",
    "                            batch_first = True, \n",
    "                            bidirectional = True)\n",
    "        self.classifier = nn.Linear(768 * 2, 2)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
    "        out, _ = self.lstm(bert_output[0])\n",
    "        logits = self.classifier(out[:, 1, :])\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad308b5e-4cc4-4e42-9013-5ec09f536d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = \"/content/drive/Shareddrives/CS682 Project/'lstm2.pt'\"\n",
    "the_final_model = torch.load(PATH1,map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624a6ed-5fac-4a2c-8897-4810c0fefc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dic = [(1996, 8249),\n",
    " (101, 7440),\n",
    " (102, 7440),\n",
    " (1997, 5349),\n",
    " (1999, 4686),\n",
    " (2000, 4605),\n",
    " (1037, 3765),\n",
    " (1998, 3358),\n",
    " (2522, 2539),\n",
    " (17258, 2406),\n",
    " (2003, 2378),\n",
    " (2005, 2195),\n",
    " (16147, 2080),\n",
    " (3572, 2016),\n",
    " (2024, 1975),\n",
    " (2008, 1649),\n",
    " (21887, 1480),\n",
    " (2013, 1431),\n",
    " (2057, 1407),\n",
    " (2015, 1348),\n",
    " (2006, 1338),\n",
    " (2047, 1275),\n",
    " (23350, 1271),\n",
    " (2007, 1205),\n",
    " (2031, 1204),\n",
    " (2004, 1117),\n",
    " (2023, 1101),\n",
    " (1521, 941),\n",
    " (2012, 924),\n",
    " (2038, 923),\n",
    " (2011, 919),\n",
    " (6677, 883),\n",
    " (2009, 868),\n",
    " (5852, 832),\n",
    " (100, 813),\n",
    " (2256, 806),\n",
    " (2045, 805),\n",
    " (2111, 794),\n",
    " (2025, 769),\n",
    " (2022, 727),\n",
    " (2062, 724),\n",
    " (2651, 706),\n",
    " (2193, 677),\n",
    " (4484, 664),\n",
    " (2163, 648),\n",
    " (2064, 640),\n",
    " (2042, 639),\n",
    " (2988, 630),\n",
    " (2102, 609),\n",
    " (2561, 581),\n",
    " (2019, 580),\n",
    " (2030, 576),\n",
    " (2001, 574),\n",
    " (2085, 553),\n",
    " (2035, 539),\n",
    " (1055, 534),\n",
    " (2040, 515),\n",
    " (2902, 507),\n",
    " (2740, 498),\n",
    " (2110, 493),\n",
    " (23713, 493),\n",
    " (1016, 485),\n",
    " (2017, 485),\n",
    " (2065, 484),\n",
    " (5604, 478),\n",
    " (2039, 476),\n",
    " (2020, 471),\n",
    " (1015, 462),\n",
    " (2097, 456),\n",
    " (2084, 454),\n",
    " (2634, 447),\n",
    " (2053, 430),\n",
    " (2021, 409),\n",
    " (2058, 400),\n",
    " (5022, 400),\n",
    " (2487, 387),\n",
    " (2027, 373),\n",
    " (2692, 372),\n",
    " (2115, 372),\n",
    " (2037, 371),\n",
    " (2951, 370),\n",
    " (2029, 362),\n",
    " (2575, 362),\n",
    " (3207, 361),\n",
    " (2055, 355),\n",
    " (2080, 354),\n",
    " (3189, 354),\n",
    " (6090, 353),\n",
    " (3231, 352),\n",
    " (4609, 347),\n",
    " (2683, 346),\n",
    " (7712, 340),\n",
    " (2420, 339),\n",
    " (1017, 334),\n",
    " (2553, 333),\n",
    " (2154, 332),\n",
    " (2549, 321),\n",
    " (17404, 321),\n",
    " (2050, 318),\n",
    " (2069, 318)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480a4f9-76bb-4895-a95f-840f34e1f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list =['cases', 'corona', 'news', 'deaths', 'tests','today','confirmed','reported','states','total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1e0df-2c85-4bcf-a377-667971188e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list = tokenizer.convert_tokens_to_ids(word_list)\n",
    "print(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ce508-da06-4762-872e-2fe7950d9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list = []\n",
    "for i in range(len(dic)):\n",
    "  ids_list.append(dic[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b68e2a-f43b-4e2f-816b-0cb77ed3171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = the_final_model.cuda()\n",
    "print(\"Validation...\")\n",
    "model.eval()\n",
    "total_eval_accuracy = 0\n",
    "total_eval_accuracy1 = 0\n",
    "total_num = 0\n",
    "tag = False\n",
    "for batch in validation_dataloader:\n",
    "    input_ids = batch[0].to(device)\n",
    "    input_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device)\n",
    "    input_list = []\n",
    "    label_list = []\n",
    "    mask_list = []\n",
    "\n",
    "    input_list_r = []\n",
    "\n",
    "    for i in range(input_ids.size(0)):\n",
    "      for ids in ids_list:\n",
    "        if ids in input_ids[i]:\n",
    "          tag = True\n",
    "      if tag == True:\n",
    "        input_list.append(input_ids[i])\n",
    "        label_list.append(labels[i])\n",
    "        mask_list.append(input_mask[i])\n",
    "        for ids in ids_list:\n",
    "          for j in range(len(input_ids[i])):\n",
    "            if input_ids[i][j] == ids:\n",
    "              input_ids[i][j] = 0\n",
    "        input_list_r.append(input_ids[i])\n",
    "      tag = False\n",
    "    \n",
    "    if len(input_list)==0:\n",
    "      continue\n",
    "\n",
    "    input_list = torch.stack((input_list)).to(device)\n",
    "    label_list = torch.stack((label_list)).to(device)\n",
    "    mask_list = torch.stack((mask_list)).to(device)\n",
    "    input_list_r = torch.stack((input_list_r)).to(device)\n",
    "\n",
    "    with torch.no_grad():        \n",
    "      out = model(input_ids = input_list, attention_mask = mask_list, token_type_ids = None)\n",
    "    pred = torch.argmax(out, dim = 1)\n",
    "    total_eval_accuracy += torch.sum(pred == label_list).item()\n",
    "\n",
    "    with torch.no_grad():        \n",
    "      out1 = model(input_ids = input_list_r, attention_mask = mask_list, token_type_ids = None)\n",
    "    pred1 = torch.argmax(out1, dim = 1)\n",
    "    total_eval_accuracy1 += torch.sum(pred1 == label_list).item()\n",
    "\n",
    "    total_num += len(input_list)\n",
    "        \n",
    "avg_val_accuracy = total_eval_accuracy / total_num\n",
    "avg_val_accuracy1 = total_eval_accuracy1 / total_num\n",
    "print(\"  Accuracy: {}\".format(avg_val_accuracy))\n",
    "print(\"  Removing Accuracy: {}\".format(avg_val_accuracy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef28d08-c38c-4c91-82a4-56ad7efef253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ae70f-18a3-40d2-ae33-4d12f8d12036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909dbbb-1123-4945-9cad-bd0bc5e7ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
